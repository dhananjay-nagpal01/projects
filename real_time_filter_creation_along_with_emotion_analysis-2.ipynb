{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fd9e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import keras.preprocessing.image as image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.utils import load_img, img_to_array\n",
    "from keras.models import  load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64108eb",
   "metadata": {},
   "source": [
    "FILTER 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ca6f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import hypot\n",
    "cap = cv2.VideoCapture(0)\n",
    "nose_image = cv2.imread(\"Documents/pig_nose.png\")\n",
    "#_, frame = cap.read()\n",
    "#ret, test_img = cap.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3b17325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter1(frame):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"Documents/shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(frame)\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray_frame, face)\n",
    "        top_nose = (landmarks.part(29).x, landmarks.part(29).y)\n",
    "        center_nose = (landmarks.part(30).x, landmarks.part(30).y)\n",
    "        left_nose = (landmarks.part(31).x, landmarks.part(31).y)\n",
    "        right_nose = (landmarks.part(35).x, landmarks.part(35).y)\n",
    "        nose_width = int(hypot(left_nose[0] - right_nose[0],left_nose[1] - right_nose[1]) * 1.7)\n",
    "        nose_height = int(nose_width * 0.55)\n",
    "        \n",
    "        # New nose position\n",
    "        top_left = (int(center_nose[0] - nose_width / 2),int(center_nose[1] - nose_height / 2))\n",
    "        bottom_right = (int(center_nose[0] + nose_width / 2),int(center_nose[1] + nose_height / 2))\n",
    "        nose_pig = cv2.resize(nose_image, (nose_width, nose_height))\n",
    "        nose_pig_gray = cv2.cvtColor(nose_pig, cv2.COLOR_BGR2GRAY)\n",
    "        _, nose_mask = cv2.threshold(nose_pig_gray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "        nose_area = frame[top_left[1]: top_left[1] + nose_height,top_left[0]: top_left[0] + nose_width]\n",
    "        nose_mask = nose_mask.astype('uint8')\n",
    "        nose_area_no_nose = cv2.bitwise_and(nose_area, nose_area, mask=nose_mask)\n",
    "        final_nose = cv2.add(nose_area_no_nose, nose_pig)\n",
    "        frame[top_left[1]: top_left[1] + nose_height,top_left[0]: top_left[0] + nose_width] = final_nose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53501f97",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2711ce39",
   "metadata": {},
   "source": [
    "FILTER 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43c592f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eye_image=cv2.imread(\"Documents/eye_patch_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386556f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter2(frame):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"Documents/shape_predictor_68_face_landmarks.dat\")\n",
    "    \n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(frame)\n",
    "    for face in faces:\n",
    "        \n",
    "        landmarks = predictor(gray_frame, face)\n",
    "        top_eye = (landmarks.part(43).x, landmarks.part(43).y)\n",
    "        top_eye_1 = (landmarks.part(44).x, landmarks.part(44).y)\n",
    "        left_eye = (landmarks.part(42).x, landmarks.part(45).y)\n",
    "        right_eye = (landmarks.part(45).x, landmarks.part(45).y)\n",
    "        bottom_eye = (landmarks.part(46).x, landmarks.part(46).y)\n",
    "        bottom_eye_1 = (landmarks.part(47).x, landmarks.part(47).y)\n",
    "        eye_width = int(hypot(left_eye[0] - right_eye[0],left_eye[1] - right_eye[1]) * 1.7)\n",
    "        eye_height = eye_width\n",
    "            # New eye position\n",
    "        top_left = (int(left_eye[0] - eye_width / 2),int(left_eye[1] - eye_height / 2))\n",
    "        bottom_right = (int(right_eye[0] + eye_width / 2),int(right_eye[1] + eye_height / 2))\n",
    "        eye_patch = cv2.resize(eye_image, (eye_width, eye_height))\n",
    "        eye_patch_gray = cv2.cvtColor(eye_patch, cv2.COLOR_BGR2GRAY)\n",
    "        _, eye_mask = cv2.threshold(eye_patch_gray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "        eye_area = frame[top_left[1]: top_left[1] + eye_height, top_left[0]: top_left[0] + eye_width]\n",
    "        eye_mask = eye_mask.astype('uint8')\n",
    "        eye_area_no_eye = cv2.bitwise_and(eye_area, eye_area, mask=eye_mask)\n",
    "        final_eye = cv2.add(eye_area_no_eye, eye_patch)\n",
    "        frame[top_left[1]: top_left[1] + eye_height,top_left[0]: top_left[0] + eye_width] = final_eye\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d626f38",
   "metadata": {},
   "source": [
    "Filter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e4a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter3(frame):\n",
    "    tongue_image = cv2.imread(\"Documents/tongue.png\")\n",
    "    #_, frame = cap.read()\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(\"Documents/shape_predictor_68_face_landmarks.dat\")\n",
    "   # _, frame = cap.read()\n",
    "        #nose_mask.fill(0)\n",
    "    gray_frame = cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(frame)\n",
    "    for face in faces:\n",
    "          landmarks = predictor(gray_frame, face)\n",
    "          upper_lip = (landmarks.part(61).x, landmarks.part(61).y)\n",
    "          upper_lip_1 = (landmarks.part(62).x, landmarks.part(62).y)\n",
    "          upper_lip_2 = (landmarks.part(63).x, landmarks.part(63).y)\n",
    "          lower_lip_1 = (landmarks.part(65).x, landmarks.part(65).y)\n",
    "          lower_lip = (landmarks.part(66).x, landmarks.part(66).y)\n",
    "          lower_lip_2 = (landmarks.part(67).x, landmarks.part(67).y)\n",
    "          lip_width = int(hypot(lower_lip_2[0] - lower_lip_1[0],lower_lip_2[1] - lower_lip_1[1]) * 1.7)\n",
    "          lip_height = int(lip_width * 0.71)\n",
    "           # New nose position\n",
    "    top_left = (int(upper_lip[0] - lip_width / 2),int(upper_lip[1] - lip_height / 2))\n",
    "    bottom_right = (int(upper_lip[0] + lip_width / 2),int(upper_lip[1] + lip_height / 2))\n",
    "    lip_tongue = cv2.resize(tongue_image, (lip_width, lip_height))\n",
    "    lip_tongue_gray = cv2.cvtColor(lip_tongue, cv2.COLOR_BGR2GRAY)\n",
    "    _, lip_mask = cv2.threshold(lip_tongue_gray, 25, 255, cv2.THRESH_BINARY_INV)\n",
    "    lip_area = test_img[top_left[1]: top_left[1] + lip_height,top_left[0]: top_left[0] + lip_width]\n",
    "    lip_mask = lip_mask.astype('uint8')\n",
    "    lip_area_no_lip = cv2.bitwise_and(lip_area, lip_area, mask=lip_mask)\n",
    "    final_lip = cv2.add(lip_area_no_lip, lip_tongue)\n",
    "    test_img[top_left[1]: top_left[1] + lip_height,top_left[0]: top_left[0] + lip_width] = final_lip\n",
    "\n",
    "        \n",
    "\n",
    "        #if (predicted_emotion == 'happy'):\n",
    "        #output=cv2.add(frame1,frame)\n",
    "        #cv2.imshow(\"Facial emotion analysis\",frame)\n",
    "        #cv2.imshow(\"lund\",output)\n",
    "            #cv2.imshow(\"Nose area\", nose_area)\n",
    "            #cv2.imshow(\"Nose pig\", nose_pig)\n",
    "            #cv2.imshow(\"final nose\", final_nose)\n",
    "        #cv2.imshow(\"Frame\", frame)\n",
    "        #key = cv2.waitKey(1)\n",
    "        #if key == 27:\n",
    "         #break\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20d7e4",
   "metadata": {},
   "source": [
    "Emotion detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d035bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"Documents/best_trained_model.h5\")\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "#cap = cv2.VideoCapture(0)\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "#predictor = dlib.shape_predictor(\"Documents/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "while True:\n",
    "    ret, test_img = cap.read()  # captures frame and returns boolean value and captured image\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces_detected:\n",
    "        cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)\n",
    "        roi_gray = gray_img[y:y + w, x:x + h]  # cropping region of interest i.e. face area from  image\n",
    "        roi_gray = cv2.resize(roi_gray, (224, 224))\n",
    "        img_pixels = img_to_array(roi_gray)\n",
    "        img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels /= 255\n",
    "\n",
    "        predictions = model.predict(img_pixels)\n",
    "\n",
    "        # find max indexed array\n",
    "        max_index = np.argmax(predictions[0])\n",
    "\n",
    "        emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        \n",
    "        predicted_emotion = emotions[max_index]\n",
    "        \n",
    "          \n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        \n",
    "\n",
    "        \n",
    "        #if (predicted_emotion == 'happy'):\n",
    "        \n",
    "            #  filter2()\n",
    "        \n",
    "    #if (predicted_emotion == 'angry'):\n",
    "     #   filter3()\n",
    "        \n",
    "    frame = cv2.resize(test_img, (1000, 700))\n",
    "    \n",
    "    #if (predicted_emotion == 'happy'):\n",
    "        \n",
    "     #      filter3(frame)\n",
    "        \n",
    "    if (predicted_emotion == 'fear'):\n",
    "        \n",
    "        filter2(frame)\n",
    "        \n",
    "        \n",
    "    #if (predicted_emotion == 'happy'):\n",
    "     #    filter3(frame)\n",
    "            \n",
    "    cv2.imshow('Facial emotion analysis',frame)    \n",
    "\n",
    "    \n",
    "    #resized_img = cv2.resize(test_img, (1000, 700))\n",
    "    #cv2.imshow('Facial emotion analysis ', resized_img)\n",
    "    \n",
    "    if cv2.waitKey(10) == ord('q'):  # wait until 'q' key is pressed\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b48c5b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f5c77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01fdb5ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d8c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install rembg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2050b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52327ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
